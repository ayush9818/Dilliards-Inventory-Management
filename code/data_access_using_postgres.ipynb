{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8f730c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install psycopg2\n",
    "# ! pip install psycopg2-binary\n",
    "#! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe07f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2577d221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema : group_11\n",
      "DBName: everything2023 HOSTNAME: pg.analytics.northwestern.edu\n"
     ]
    }
   ],
   "source": [
    "CURR_DIR=os.getcwd()\n",
    "#print(CURR_DIR)\n",
    "env_file_path=os.path.join(CURR_DIR, '.env')\n",
    "#print(env_file_path)\n",
    "\n",
    "# load the environment variables\n",
    "load_dotenv(env_file_path)\n",
    "env=os.environ\n",
    "\n",
    "DBNAME=env.get('DBNAME')\n",
    "USERNAME=env.get('USERNAME')\n",
    "PASSWORD=env.get('PASSWORD')\n",
    "HOSTNAME=env.get('HOSTNAME')\n",
    "PORT=env.get('PORT')\n",
    "SCHEMA=env.get('SCHEMA')\n",
    "print(f\"Schema : {SCHEMA}\")\n",
    "print(f\"DBName: {DBNAME} HOSTNAME: {HOSTNAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f14dee61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish the connection\n",
    "conn = psycopg2.connect(\n",
    "    dbname=DBNAME,\n",
    "    user=USERNAME,\n",
    "    password=PASSWORD,\n",
    "    host=HOSTNAME,\n",
    "    port=PORT\n",
    ")\n",
    "\n",
    "# Create a cursor object to interact with the database\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Setting the SearchPath to Group Schema\n",
    "cur.execute(f\"SET search_path TO {SCHEMA};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38e1758d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DEPT', 'DEPTDESC']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEPT</th>\n",
       "      <th>DEPTDESC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>800</td>\n",
       "      <td>CLINIQUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>801</td>\n",
       "      <td>LESLIE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1100</td>\n",
       "      <td>GARY F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1107</td>\n",
       "      <td>JACQUES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1202</td>\n",
       "      <td>CABERN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1301</td>\n",
       "      <td>BE2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1704</td>\n",
       "      <td>R LAUREN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1905</td>\n",
       "      <td>R &amp; Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2102</td>\n",
       "      <td>CAB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2105</td>\n",
       "      <td>R TAYLOR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DEPT  DEPTDESC\n",
       "0   800  CLINIQUE\n",
       "1   801    LESLIE\n",
       "2  1100    GARY F\n",
       "3  1107   JACQUES\n",
       "4  1202    CABERN\n",
       "5  1301       BE2\n",
       "6  1704  R LAUREN\n",
       "7  1905     R & Y\n",
       "8  2102       CAB\n",
       "9  2105  R TAYLOR"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting Data from DeptInfo\n",
    "cur.execute(\"SELECT * FROM deptinfo LIMIT 10;\")\n",
    "column_names = [desc[0] for desc in cur.description]\n",
    "print(column_names)\n",
    "\n",
    "rows = cur.fetchall()\n",
    "df_deptinfo = pd.DataFrame(rows, columns=column_names)\n",
    "df_deptinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1789c15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SKU', 'STORE', 'COST', 'RETAIL']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SKU</th>\n",
       "      <th>STORE</th>\n",
       "      <th>COST</th>\n",
       "      <th>RETAIL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>102</td>\n",
       "      <td>123.36</td>\n",
       "      <td>440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>103</td>\n",
       "      <td>123.36</td>\n",
       "      <td>440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>104</td>\n",
       "      <td>123.36</td>\n",
       "      <td>440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>202</td>\n",
       "      <td>123.36</td>\n",
       "      <td>440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>203</td>\n",
       "      <td>123.36</td>\n",
       "      <td>440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>204</td>\n",
       "      <td>123.36</td>\n",
       "      <td>440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>302</td>\n",
       "      <td>123.36</td>\n",
       "      <td>440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>304</td>\n",
       "      <td>123.36</td>\n",
       "      <td>440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>307</td>\n",
       "      <td>123.36</td>\n",
       "      <td>440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>309</td>\n",
       "      <td>123.36</td>\n",
       "      <td>440.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SKU  STORE    COST  RETAIL\n",
       "0    3    102  123.36   440.0\n",
       "1    3    103  123.36   440.0\n",
       "2    3    104  123.36   440.0\n",
       "3    3    202  123.36   440.0\n",
       "4    3    203  123.36   440.0\n",
       "5    3    204  123.36   440.0\n",
       "6    3    302  123.36   440.0\n",
       "7    3    304  123.36   440.0\n",
       "8    3    307  123.36   440.0\n",
       "9    3    309  123.36   440.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting Data from SKSTINFO\n",
    "cur.execute(\"SELECT * FROM skstinfo LIMIT 10;\")\n",
    "column_names = [desc[0] for desc in cur.description]\n",
    "print(column_names)\n",
    "\n",
    "rows = cur.fetchall()\n",
    "df_skstinfo = pd.DataFrame(rows, columns=column_names)\n",
    "df_skstinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81d015a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "InFailedSqlTransaction",
     "evalue": "current transaction is aborted, commands ignored until end of transaction block\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInFailedSqlTransaction\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Getting Data from SKUINFO\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM skuinfo LIMIT 10;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m column_names \u001b[38;5;241m=\u001b[39m [desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m desc \u001b[38;5;129;01min\u001b[39;00m cur\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(column_names)\n",
      "\u001b[0;31mInFailedSqlTransaction\u001b[0m: current transaction is aborted, commands ignored until end of transaction block\n"
     ]
    }
   ],
   "source": [
    "# Getting Data from SKUINFO\n",
    "cur.execute(\"SELECT * FROM skuinfo LIMIT 10;\")\n",
    "column_names = [desc[0] for desc in cur.description]\n",
    "print(column_names)\n",
    "\n",
    "rows = cur.fetchall()\n",
    "df_skuinfo = pd.DataFrame(rows, columns=column_names)\n",
    "df_skuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9b00dfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['STORE', 'CITY', 'STATE', 'ZIP']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STORE</th>\n",
       "      <th>CITY</th>\n",
       "      <th>STATE</th>\n",
       "      <th>ZIP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>ST. PETERSBURG</td>\n",
       "      <td>FL</td>\n",
       "      <td>33710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>ST. LOUIS</td>\n",
       "      <td>MO</td>\n",
       "      <td>63126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>LITTLE ROCK</td>\n",
       "      <td>AR</td>\n",
       "      <td>72201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>FORT WORTH</td>\n",
       "      <td>TX</td>\n",
       "      <td>76137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>TEMPE</td>\n",
       "      <td>AZ</td>\n",
       "      <td>85281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60</td>\n",
       "      <td>ASPEN</td>\n",
       "      <td>CO</td>\n",
       "      <td>81611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>62</td>\n",
       "      <td>TULSA</td>\n",
       "      <td>OK</td>\n",
       "      <td>74110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>65</td>\n",
       "      <td>SAN ANTONIO</td>\n",
       "      <td>TX</td>\n",
       "      <td>78218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>67</td>\n",
       "      <td>DALLAS</td>\n",
       "      <td>TX</td>\n",
       "      <td>75207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100</td>\n",
       "      <td>SALISBURY</td>\n",
       "      <td>NC</td>\n",
       "      <td>28146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   STORE            CITY STATE    ZIP\n",
       "0      2  ST. PETERSBURG    FL  33710\n",
       "1      3       ST. LOUIS    MO  63126\n",
       "2      4     LITTLE ROCK    AR  72201\n",
       "3      7      FORT WORTH    TX  76137\n",
       "4      9           TEMPE    AZ  85281\n",
       "5     60           ASPEN    CO  81611\n",
       "6     62           TULSA    OK  74110\n",
       "7     65     SAN ANTONIO    TX  78218\n",
       "8     67          DALLAS    TX  75207\n",
       "9    100       SALISBURY    NC  28146"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting Data from STRINFO\n",
    "cur.execute(\"SELECT * FROM strinfo LIMIT 10;\")\n",
    "column_names = [desc[0] for desc in cur.description]\n",
    "print(column_names)\n",
    "\n",
    "rows = cur.fetchall()\n",
    "df_strinfo = pd.DataFrame(rows, columns=column_names)\n",
    "df_strinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "33a2ae89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SKU', 'STORE', 'COST', 'RETAIL', 'SKU', 'DEPT', 'CLASSID', 'UPC', 'STYLE', 'COLOR', 'SIZE', 'PACKSIZE', 'VENDOR', 'BRAND']\n",
      "    SKU  STORE  COST  RETAIL   SKU  DEPT CLASSID           UPC         STYLE   \n",
      "0  1918   9806  15.0   19.00  1918  9105     111  400001918000  21B   S45KR4  \\\n",
      "1  4220    102   7.0    4.99  4220  2102     979  400004220000        N51202   \n",
      "2  4220    103   7.0    9.99  4220  2102     979  400004220000        N51202   \n",
      "3  4220    309   7.0    5.00  4220  2102     979  400004220000        N51202   \n",
      "4  4220    502   7.0    5.00  4220  2102     979  400004220000        N51202   \n",
      "5  4220    503   7.0    9.99  4220  2102     979  400004220000        N51202   \n",
      "6  4220    603   7.0    9.99  4220  2102     979  400004220000        N51202   \n",
      "7  4220    703   7.0    9.99  4220  2102     979  400004220000        N51202   \n",
      "8  4220    802   7.0    5.00  4220  2102     979  400004220000        N51202   \n",
      "9  4220    809   7.0    5.00  4220  2102     979  400004220000        N51202   \n",
      "\n",
      "        COLOR SIZE  PACKSIZE   VENDOR     BRAND  \n",
      "0  LIGHT SAGE   4X         1  3916215  ROUNDTRE  \n",
      "1        PINK  36B         1  1446212  CABERNET  \n",
      "2        PINK  36B         1  1446212  CABERNET  \n",
      "3        PINK  36B         1  1446212  CABERNET  \n",
      "4        PINK  36B         1  1446212  CABERNET  \n",
      "5        PINK  36B         1  1446212  CABERNET  \n",
      "6        PINK  36B         1  1446212  CABERNET  \n",
      "7        PINK  36B         1  1446212  CABERNET  \n",
      "8        PINK  36B         1  1446212  CABERNET  \n",
      "9        PINK  36B         1  1446212  CABERNET  \n"
     ]
    }
   ],
   "source": [
    "# Using JOIN BASED QUERY\n",
    "QUERY1 = \"\"\"\n",
    "SELECT *\n",
    "FROM skstinfo as x\n",
    "inner join \n",
    "skuinfo as y \n",
    "on x.\"SKU\" = y.\"SKU\"\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "try:\n",
    "    cur.execute(QUERY1)\n",
    "    column_names = [desc[0] for desc in cur.description]\n",
    "    print(column_names)\n",
    "\n",
    "    rows = cur.fetchall()\n",
    "    df_query1= pd.DataFrame(rows, columns=column_names)\n",
    "    print(df_query1)\n",
    "except psycopg2.Error as e:\n",
    "    print(f\"Error executing command: {e}\")\n",
    "    conn.rollback()  # Rollback the current transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8c9c467",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd55435f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_pyspark.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_pyspark.py\n",
    "from pyspark.sql import SparkSession\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "CURR_DIR=os.getcwd()\n",
    "#print(CURR_DIR)\n",
    "env_file_path=os.path.join(CURR_DIR, '.env')\n",
    "#print(env_file_path)\n",
    "\n",
    "# load the environment variables\n",
    "load_dotenv(env_file_path)\n",
    "env=os.environ\n",
    "\n",
    "DBNAME=env.get('DBNAME')\n",
    "USERNAME=env.get('USERNAME')\n",
    "PASSWORD=env.get('PASSWORD')\n",
    "HOSTNAME=env.get('HOSTNAME')\n",
    "PORT=env.get('PORT')\n",
    "SCHEMA=env.get('SCHEMA')\n",
    "print(f\"Schema : {SCHEMA}\")\n",
    "print(f\"DBName: {DBNAME} HOSTNAME: {HOSTNAME}\")\n",
    "\n",
    "JAR_PATH = \"/Users/dare_devil/Desktop/MLDS_2024/Quarter1/Everything_Starts_With_Data/postgresql-42.6.0.jar\"\n",
    "# .config(\"spark.jars\", JAR_PATH) \\\n",
    "def create_spark_session():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PostgresApp\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.2.18\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "def read_from_postgres(spark, jdbc_url, table_name):\n",
    "    df = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .option(\"user\", USERNAME) \\\n",
    "        .option(\"password\", PASSWORD) \\\n",
    "        .load()\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark_session = create_spark_session()\n",
    "\n",
    "    # Update these details\n",
    "    jdbc_url = f\"jdbc:postgresql://{HOSTNAME}:5432/{DBNAME}\"\n",
    "    table_name = \"group_11.deptinfo\"\n",
    "\n",
    "    dataframe = read_from_postgres(spark_session, jdbc_url, table_name)\n",
    "    dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7df9870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(JAR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08a4b17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/23 10:43:29 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f\"--jars  {spark_jar_path} pyspark-shell\"\n",
    "\n",
    "\n",
    "##.config(\"spark.executor.extraClassPath\", spark_jar_path) \\\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PostgreSQL with PySpark in Jupyter\") \\\n",
    "    .config(\"spark.jars\", f\"{spark_jar_path}\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "database_url = \n",
    "database_properties = {\"user\": USERNAME, \"password\": PASSWORD, \"driver\": \"org.postgresql.Driver\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4db43c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o39.jdbc.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:588)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:41)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatabase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgroup_11.deptinfo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase_properties\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/Desktop/MLDS_2024/Quarter1/Everything_Starts_With_Data/spark-3.5.0-bin-hadoop3/python/pyspark/sql/readwriter.py:946\u001b[0m, in \u001b[0;36mDataFrameReader.jdbc\u001b[0;34m(self, url, table, column, lowerBound, upperBound, numPartitions, predicates, properties)\u001b[0m\n\u001b[1;32m    944\u001b[0m     jpredicates \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mtoJArray(gateway, gateway\u001b[38;5;241m.\u001b[39mjvm\u001b[38;5;241m.\u001b[39mjava\u001b[38;5;241m.\u001b[39mlang\u001b[38;5;241m.\u001b[39mString, predicates)\n\u001b[1;32m    945\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mjdbc(url, table, jpredicates, jprop))\n\u001b[0;32m--> 946\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjprop\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Desktop/MLDS_2024/Quarter1/Everything_Starts_With_Data/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/MLDS_2024/Quarter1/Everything_Starts_With_Data/spark-3.5.0-bin-hadoop3/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Desktop/MLDS_2024/Quarter1/Everything_Starts_With_Data/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o39.jdbc.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:588)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:41)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.jdbc(database_url, \"group_11.deptinfo\", properties=database_properties)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ca4c857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "245a18d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f189465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
